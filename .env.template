OPENAI_API_KEY=

# OPTIONAL: Set the Anthropic API key if you still want to use Anthropic models
# (see the explanation to the REMAP_* variables below).
#ANTHROPIC_API_KEY=

# OPTIONAL: Authentication for the proxy (recommended when exposing the proxy
# beyond your machine). Set a strong random key to require clients to
# authenticate to the LiteLLM proxy.
#
# When set, clients must present this key as their API key when calling the
# proxy. For Claude Code CLI, you can pass it inline like so:
# ```
#   ANTHROPIC_API_KEY="<LITELLM_MASTER_KEY>" \
#   ANTHROPIC_BASE_URL=http://localhost:4000 \
#   claude
# ````
# (If you've previously signed in with Claude Code CLI, run `claude /logout`.)
#LITELLM_MASTER_KEY=

# OPTIONAL: Change model remaps if you need to (the values you see below are
# the defaults).
#
# Also, if you do not want to remap one or more Claude models at all and want
# to keep using the original ones by Anthropic, uncomment the respective
# variables, but delete their values (leave nothing to the right of the equal
# signs), or explicitly set them to map back to the Anthropic models.
#
# NOTE: Remapping is done not only for Claude Haiku and Sonnet, but also for
# Opus, because /agents in Claude Code CLI don't always inherit the model
# choice made for the CLI globally, they can also be configured individually.
# Moreover, the model choices for the built-in agents are hardcoded and cannot
# be changed. For these reasons it is "safer" to remap all models.
#REMAP_CLAUDE_HAIKU_TO=gpt-5-mini-reason-minimal
#REMAP_CLAUDE_SONNET_TO=gpt-5-reason-medium
#REMAP_CLAUDE_OPUS_TO=gpt-5-reason-high

# OPTIONAL: Override base URLs if needed
#OPENAI_BASE_URL=https://api.openai.com/v1
#ANTHROPIC_BASE_URL=https://api.anthropic.com

# OPTIONAL: You can turn off the prompt that is injected for non-Claude models
# to force them to use only one tool at a time.
#
# NOTE: Turning this prompt injection off is NOT recommended when remapping to
# the GPT-5 model, because with medium and high reasoning efforts it attempts
# to use multiple tools at once quite often, causing Claude Code CLI to
# silently stop processing the request (the CLI does not support multiple tool
# calls in a single model response).
#ENFORCE_ONE_TOOL_CALL_PER_RESPONSE=true

# OPTIONAL: Whether to always convert ChatCompletions API interactions to
# Responses API, regardless of the model (true), or only when necessary (false
# or unset, which is the default and is RECOMMENDED).
#
# In the latter case, Responses API will be used if the model is GPT-5-Codex,
# because this model does not support ChatCompletions API. Otherwise, no
# conversion will occur.
#
# ATTENTION: Support of Responses API (and, subsequently, GPT-5-Codex) is still
# WORK IN PROGRESS and is completely unreliable. This will be addressed in
# future versions (at which point this paragraph of the comment will be
# removed).
#ALWAYS_USE_RESPONSES_API=false

# OPTIONAL: Langfuse configuration for logging LiteLLM request/response traces.
# Useful for debugging.
#
# NOTE: If you set these keys, make sure to install Langfuse with either
# `uv sync --all-extras` or `uv sync --extra langfuse`
#LANGFUSE_SECRET_KEY="sk-..."
#LANGFUSE_PUBLIC_KEY="pk-..."
#LANGFUSE_HOST="https://cloud.langfuse.com"

# OPTIONAL: Alternative logging of LiteLLM request/response traces (as well as
# potential conversions between ChatCompletions API and Responses API) in the
# form of local markdown files written to `.traces/` folder. Makes it easier to
# feed the traces into AI Coding Assistants to fix things.
#WRITE_TRACES_TO_FILES=false
