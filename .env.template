# LLM provider specific environment variables. Check out the following resource
# to learn more about all the providers (and, consequently, environment
# variables) LiteLLM supports: https://docs.litellm.ai/docs/providers
OPENAI_API_KEY=

# OPTIONAL: Set the Anthropic API key if you still want to use Anthropic models
# (see the explanation to the REMAP_* variables below).
#ANTHROPIC_API_KEY=

# OPTIONAL: Override base URLs if needed
#OPENAI_BASE_URL=https://api.openai.com/v1
#ANTHROPIC_BASE_URL=https://api.anthropic.com

# OPTIONAL: Authentication for the LiteLLM server (recommended when exposing
# the server beyond your machine). Set a strong random key to require clients
# to authenticate to the LiteLLM server.
#
# When set, clients must present this key as their API key when calling the
# proxy. For Claude Code CLI, you can pass it inline like so:
# ```
#   ANTHROPIC_API_KEY="<LITELLM_MASTER_KEY>" \
#   ANTHROPIC_BASE_URL=http://localhost:4000 \
#   claude
# ```
# (If you've previously signed in with Claude Code CLI, run `claude /logout`.)
#LITELLM_MASTER_KEY=

# OPTIONAL: Change model remaps if you need to (the values you see below are
# the defaults).
#
# NOTE: If you don't want to remap one or more Claude models at all and want to
# keep using the original models by Anthropic, uncomment the respective
# variables, but delete their values (leave nothing to the right of the equal
# sign). Alternatively, you can set them to map back to the Anthropic models of
# your choosing explicitly.
#REMAP_CLAUDE_HAIKU_TO=gpt-5.1-codex-mini-reason-none
#REMAP_CLAUDE_SONNET_TO=gpt-5-codex-reason-medium
#REMAP_CLAUDE_OPUS_TO=gpt-5.1-reason-high

# OPTIONAL: You can turn off the prompt injection that forces non-Claude models
# to use only one tool at a time.
#
# ATTENTION: Turning it off is NOT recommended. GPT-5, when used with medium or
# high reasoning effort and without such injection, attempts to make multiple
# tool calls at once quite often, and that causes Claude Code CLI to silently
# stop processing the request (the CLI does not support multiple tool calls in
# a single response).
#ENFORCE_ONE_TOOL_CALL_PER_RESPONSE=false

# OPTIONAL: Whether to convert ChatCompletions API requests to Responses API
# format for ALL non-Claude models (true), or only for the OpenAI models that
# don't support ChatCompletions API (false or unset, RECOMMENDED).
#
# The OpenAI models that don't support ChatCompletions API are: GPT-5-Codex,
# GPT-5.1 Codex, GPT-5 Pro and few others (see https://platform.openai.com/docs/models)
#ALWAYS_USE_RESPONSES_API=true

# OPTIONAL: Langfuse configuration for logging LiteLLM request/response traces.
# Useful for debugging.
#
# NOTE: If you set these keys, make sure to install Langfuse with either
# `uv sync --all-extras` or `uv sync --extra langfuse`
#LANGFUSE_SECRET_KEY="sk-..."
#LANGFUSE_PUBLIC_KEY="pk-..."
#LANGFUSE_HOST="https://cloud.langfuse.com"

# OPTIONAL: Alternative logging of LiteLLM request/response traces (as well as
# potential conversions between ChatCompletions API and Responses API) in the
# form of local markdown files written to `.traces/` folder. Makes it easier to
# feed the traces into AI Coding Assistants to fix things.
#WRITE_TRACES_TO_FILES=true

PYTHONUNBUFFERED=1
